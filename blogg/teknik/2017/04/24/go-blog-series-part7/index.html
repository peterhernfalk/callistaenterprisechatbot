<!DOCTYPE html>
<html lang="sv-se">
<head>
    <meta charset="utf-8">

    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="description" content="Callista Enterprise - seniora IT-arkitekter och systemutvecklare inom Java, öppen källkod, agil utveckling och systemintegration">
    <!--<meta name="viewport" content="width=device-width, initial-scale=1">-->
    <meta name="viewport" content="initial-scale=1.0001, minimum-scale=1.0001, maximum-scale=1.0001, user-scalable=no"/>

    <link rel="icon" href="../../../../../../images/icons/callista_favicon.svg" type="image/x-icon" />
    <link rel="shortcut icon" href="../../../../../../images/icons/callista_favicon.svg" type="image/x-icon" />
    <link rel="apple-touch-icon" href="../../../../../../images/icons/callista_favicon_160x160.png">

    <title>Go microservices, part 7 - Service Discovery & Load-balancing. | Callista</title>

    <link rel="stylesheet" href="../../../../../../css/style.css%3Fv=1.css">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for callistaenterprise.se" href="../../../../../../feed.xml" />

    <!--[if lte IE 8]>
    <style>* { display: none !important; }</style>
    <meta http-equiv="refresh" content="0; url=/oldie/"/>
    <![endif]-->

    <script type="text/javascript" src="https://use.typekit.net/kig5egm.js"></script>
    <script type="text/javascript">
        try {
            Typekit.load({ async: true });
        } catch (e) {
        }
    </script>

    <!-- Facebook Pixel Code -->
    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
     fbq('init', '804675599711872');
     fbq('track', 'PageView');
    </script>
    <noscript>
     <img height="1" width="1" src="https://www.facebook.com/tr?id=804675599711872&ev=PageView&noscript=1"/>
    </noscript>
    <!-- End Facebook Pixel Code -->

</head>
<body>

    <header class="ce-header" data-scroll="0">
    <nav>
        <img alt="menubg" class="ce-menu-icon" style="z-index:3; top: 0px; width: 50px; height: 100%;" src="../../../../../../images/icons/menu_background.png">
        <a class="ce-logotype" href="../../../../../../index.html" style="z-index: 2;">
            <img alt="Callista" style="max-width: 100%; height: auto;" src="../../../../../../images/logotype/callista_small2.svg">
        </a>
        <table>
            <tr>
                <td>
            <ul id="menu-primary" class="ce-menu-primary">
                
                    
                            <li><a href="../../../../../../om/index.html">Om oss</a></li>
                    
                
                    
                            <li><a href="../../../../../../erbjudanden.html">Erbjudanden</a></li>
                    
                
                    
                            <li><a href="../../../../../../event.html">Event</a></li>
                    
                
                    
                        
                            <li><a class="ce-active" href="../../../../../../blogg.html">Blogg</a></li>
                        
                    
                
                    
                            <li><a href="../../../../../../om/jobb/index.html">Jobba hos oss</a></li>
                    
                
            </ul>
                </td>

                <td>
            <ul class="ce-menu-secondary">
                
                <li><a href="../../../../../../english/index.html">English</a></li>
                
            </ul>
                </td>
            </tr>
        </table>
            <a id="menu" class="ce-menu-icon" href="../go-blog-series-part7.html#" style="z-index: 4;">
                <img alt="Visa meny" src="../../../../../../images/icons/menu.png">
            </a>
            <a class="ce-search-icon" href="../go-blog-series-part7.html#">
                <img alt="Sök" src="../../../../../../images/icons/search.png">
            </a>

        </table>
    </nav>
</header>


    <div class="ce-main">
        <section class="ce-start lazyImg">
    <article>
        <h1>Blogg</h1>
        <p class="ce-ingress">
            Här finns tekniska artiklar, presentationer och nyheter om arkitektur och systemutveckling. Håll dig uppdaterad, följ oss på <a class="ce-active" href="http://twitter.com/callistaent">Twitter</a>
        </p>
    </article>
</section>

<section class="ce-section">
    <div class="ce-content">
        
        

        
        

        <article class="ce-blog">
            <header>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <img alt="Callista medarbetare Erik Lupander" src="../../../../../../assets/medarbetare/eriklupander_mini.png">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <h2>
        <a href="../go-blog-series-part7.html">Go microservices, part 7 - Service Discovery & Load-balancing.</a>
        
        
    </h2>
    <h3>
        <time datetime="2017-04-24T00:00:00+00:00">
            24 April 2017
        </time>
        //
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        

        
        
        <a href="../../../../../../om/medarbetare/eriklupander/index.html">Erik Lupander</a>
        

        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    </h3>
    
</header>




<p>This part of the <a href="../../../02/17/go-blog-series-part1.html">blog series</a> will deal with two fundamental pieces of a sound microservice architecture - service discovery and load-balancing - and how they facilitate the kind of horizontal scaling we usually state as an important non-functional requirement in 2017.</p>

<h1 id="introduction">Introduction</h1>
<p>While load-balancing is a rather well-known concept, I think Service Discovery entails a more in-depth explanation. I’ll start with a question:</p>

<p><em>“How does Service A talk to Service B without having any knowledge about where to find Service B?”</em></p>

<p>In other words - if we have 10 instances of Service B running on an arbitrary number of cluster nodes, someone needs to keep track of all these 10 instances. So when Service A needs to communicate with Service B, at least one proper IP address or hostname for an instance of Service B must be made available to Service A (client-side load balancing) - or - Service A must be able to delegate the address resolution and routing to a 3rd party given a known logical name of Service B (server-side load balancing). In the continuously changing context of a microservice landscape, either approach requires <em>Service Discovery</em> to be present. In its simplest form, Service Discovery is just a registry of running instances for one or many services.</p>

<p>If this sounds a lot like a DNS service to you, it kind of is. The difference being that service discovery is for use within your internal cluster so your microservices can find each other, while DNS typically is for more static and external routing so external parties can have requests routed to your service(s). Also, DNS servers and the DNS protocol are typically not well suited for handling the volatile nature of microservice environments with ever-changing topology with containers and nodes coming and going, clients often not honoring TTL values, failure detection etc.</p>

<p>Most microservice frameworks provides one or several options for service discovery. By default, Spring Cloud / Netflix OSS uses <a href="https://github.com/Netflix/eureka">Netflix Eureka</a> (also supports Consul, etcd and ZooKeeper) where services register themselves with a known Eureka instance and then intermittently sends heartbeats to make sure the Eureka instance(s) know they’re still alive. An option (written in Go) that’s becoming more popular is <a href="https://www.consul.io/">Consul</a> that provides a rich feature set including an integrated DNS. Other popular options are the use of distributed and replicable key-value stores such as <a href="https://github.com/coreos/etcd">etcd</a> where services can register themselves. Apache <a href="https://zookeeper.apache.org/">ZooKeeper</a> should also be mentioned in this crowd.</p>

<p>In this blog post, we’ll primarily deal with the mechanisms offered by “Docker Swarm” (e.g. Docker in swarm mode) and showcase the <em>service</em> abstraction that we explored in <a href="../../../03/09/go-blog-series-part5.html">part 5</a> of the blog series and how it actually provides us with both service discovery and server-side load-balancing. Additionally, we’ll take a look at mocking of outgoing HTTP requests in our unit tests using <a href="https://github.com/h2non/gock">gock</a> since we’ll be doing service-to-service communication.</p>

<p><em>Note: When referring to “Docker Swarm” in this blog series, I am referring to running Docker 1.12 or later in <a href="https://docs.docker.com/engine/swarm/">swarm mode</a>. “<a href="https://docs.docker.com/swarm/">Docker Swarm</a>” as a standalone concept was discontinued with the release of Docker 1.12.</em></p>

<h2 id="two-types-of-load-balancers">Two types of load-balancers</h2>
<p>In the realm of microservices, one usually differentiates between the two types of load-balancing mentioned above:</p>

<ul>
  <li>
    <p>Client-side: It’s up to the client to query a discovery service to get actual address information (IPs, hostnames, ports) of services they need to call, from which they then pick one using a load-balancing strategy such as round-robin or random. Also, in order to not have to query the discovery service for each upcoming invocation, each client typically keeps a local cache of endpoints that has to be kept in reasonable sync with the master info from the discovery service. An example of a client-side load balancer in the Spring Cloud ecosystem is <a href="https://github.com/Netflix/ribbon">Netflix Ribbon</a>. Something <a href="https://github.com/go-kit/kit/issues/68">similar</a> exists in the <a href="https://github.com/go-kit/kit">go-kit</a> ecosystem that’s backed by etcd. Some advantages of client-side load-balancing is resilience, decentralization and no central bottlenecks since each service consumer keeps its own registry of producer endpoints. Some drawbacks are higher internal service complexity and risk of local registries containing stale entries. 
 <img src="../../../../../../assets/blogg/goblog/part7-clientsidelb.png" alt="client-side" /></p>
  </li>
  <li>
    <p>Server-side: In this model, the client relies on the load-balancer to look up a suitable instance of the service it wants to call given a logical name for the target service. This mode of operation is often referred to as “proxy” since it functions both as a load-balancer and a reverse-proxy. I’d say the main advantage here is simplicity. The load-balancer and service discovery mechanism is typically built into your container orchestrator and you don’t have to care about installing or managing those components. Also, the client (e.g. our service) doesn’t have to be aware of the service registry - the load-balancer takes care of that for us. Being reliant on the load-balancer to route all calls arguably decreases resilience and the load-balancer <em>could</em> theoretically become a performance bottleneck.
<img src="../../../../../../assets/blogg/goblog/part7-serversidelb.png" alt="server-side" /></p>
  </li>
</ul>

<p>Note that the actual <em>registration</em> of producer services in the server-side example above is totally transparent to you as developer when we’re using the <em>service</em> abstraction of Docker in swarm mode. I.e - our producer services isn’t even aware they are operating in a server-side load-balanced context (or even in the context of a container orchestrator). Docker in swarm mode takes care of the full registration/heartbeat/deregistration for us.</p>

<p>In the example domain we’ve been working with since <a href="../../../02/21/go-blog-series-part2.html">part 2</a> of the <a href="../../../02/17/go-blog-series-part1.html">blog series</a>, we might want to ask our <em>accountservice</em> to fetch a random quote-of-the-day from the <em>quotes-service</em>. In this blog post, we’ll concentrate using Docker Swarm mechanics for service discovery and load-balancing. If you’re interested in how to integrate a Go-based microservice with Eureka, I wrote a <a href="../../../../2016/05/27/building-a-microservice-with-golang/index.html">blog-post</a> including that subject in 2016. I’ve also authored a simplistic and opinionated <a href="https://github.com/eriklupander/eeureka">client-side library</a> to integrate Go apps with Eureka including basic lifecycle management.</p>

<h1 id="consuming-service-discovery-information">Consuming service discovery information</h1>
<p>Let’s say you want to build a custom-made monitoring application and need to query the <em>/health</em> endpoint of every instance of every deployed service. How would your monitoring app know what IP’s and ports to query? You need to get hold of actual service discovery details. If you’re using Docker Swarm as your service discovery and load-balancing provider and need those IPs, how would you get hold of the IP address of each instance when Docker Swarm is keeping that information for us? With a client-side solution such as Eureka you’d just consume the information using its API. However, in the case of relying on the orchestrator’s service discovery mechanisms, this may not be as straightforward. I’d say there’s one primary option to pursue and a few secondary options one could consider for more specific use cases.</p>

<h5 id="docker-remote-api">Docker Remote API</h5>
<p>Primarily, I would recommend using the Docker Remote API - e.g. use the Docker APIs from within your services to query the Swarm Manager for service and instance information. After all, if you’re using your container orchestrator’s built-in service discovery mechanism, that’s the source you should be querying. For portability, if that’s an issue, one can always write an adapter for your Orchestrator of choice. However, it should be stated that using the Orchestrator’s API have some caveats too - it ties your solution closely to a specific container API and you’d have to make sure your application can talk to the Docker Manager(s), e.g. they’d be aware of a bit more of the context they’re running in and using the Docker Remote API does increase service complexity somewhat.</p>

<h5 id="alternatives">Alternatives</h5>
<ul>
  <li>Use an additional separate service discovery mechanism - i.e. run Netflix Eureka, Consul or similar and make sure microservices that wants to be made discoverable register/deregister themselves there in addition to the Docker swarm mode mechanics. Then just use use the discovery service’s API for registering/querying/heartbeating etc. I dislike this option as it introduces complexity into services when Docker in swarm mode can handle so much of this for us more or less transparently. I almost consider this option an anti-pattern so don’t do this unless you really have to.</li>
  <li>Application-specific discovery tokens - in this approach, services that want to broadcast their existence can periodically post a “discovery token” with IP, service name etc. on a message topic. Consumers that needs to know about instances and their IPs can subscribe to the topic and keep its own registry of service instances up-to date. When we look at Netflix Turbine <em>without</em> Eureka in a later blog-post, we’ll use this mechanism to feed information to a <a href="https://github.com/eriklupander/turbine-amqp-plugin">custom Turbine discovery plugin</a> I’ve created by letting Hystrix stream producers register themselves with Turbine using discovery tokens. This approach is a bit different as it doesn’t really have to leverage the full service registry - after all, in this particular use-case we only care about a specific set of services.</li>
</ul>

<h1 id="source-code">Source code</h1>

<p>Feel free to checkout the appropriate branch for the completed source code of this part from <a href="https://github.com/callistaenterprise/goblog/tree/P7">github</a>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git checkout P7
</code></pre></div></div>

<p><em>Note: Most of the Go source code for the blog series was rewritten in July 2019 to better reflect contemporary idiomatic Go coding guidelines and design patterns. However, the corresponding <a href="https://github.com/callistaenterprise/goblog/tree/P7">git branch</a> for each part of the series remains unchanged in order to stay aligned with the content of each installment. For the latest and greatest code, look at the <a href="https://github.com/callistaenterprise/goblog">master</a> branch in github.</em></p>

<h1 id="scaling-and-load-balancing">Scaling and load-balancing</h1>

<p>We’ll continue this part by taking a look at scaling our “accountservice” microservice to run multiple instances and see if we can make Docker Swarm automatically load-balance requests to it for us.</p>

<p>In order to know what instance that actually served a request we’ll add a new field to the “Account” struct that we can populate with the IP address of the producing service instance. Open <em>/accountservice/model/account.go</em>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>type Account struct {
        Id string `json:"id"`
        Name string  `json:"name"`
        
        // NEW
        ServedBy string `json:"servedBy"`
}
</code></pre></div></div>

<p>When serving an Account in the <em>GetAccount</em> function, we’ll now populate the <em>ServedBy</em> field before returning. Open <em>/accountservice/service/handlers.go</em> and add the <em>GetIp()</em> function as well as the line of code that populates the <em>ServedBy</em> field on the struct:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>func GetAccount(w http.ResponseWriter, r *http.Request) {
    
	// Read the 'accountId' path parameter from the mux map
	var accountId = mux.Vars(r)["accountId"]
    
    // Read the account struct BoltDB
	account, err := DBClient.QueryAccount(accountId)
    
    account.ServedBy = getIP()      // NEW, add this line
    ...
}

// ADD THIS FUNC
func getIP() string {
        addrs, err := net.InterfaceAddrs()
        if err != nil {
                return "error"
        }
        for _, address := range addrs {
                // check the address type and if it is not a loopback the display it
                if ipnet, ok := address.(*net.IPNet); ok &amp;&amp; !ipnet.IP.IsLoopback() {
                        if ipnet.IP.To4() != nil {
                                return ipnet.IP.String()
                        }
                }
        }
        panic("Unable to determine local IP address (non loopback). Exiting.")
}
</code></pre></div></div>

<p>The <em>getIP()</em> function should go into some “utils” package since it’s reusable and useful for a number of different occurrences when we need to determine the non-loopback IP-address of a running service.</p>

<p>Rebuild and redeploy our service by running <em>copyall.sh</em> again from <em>$GOPATH/src/github.com/callistaenterprise/goblog</em>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; ./copyall.sh
</code></pre></div></div>

<p>Wait until it’s finished and then type:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; docker service ls
ID            NAME             REPLICAS  IMAGE
yim6dgzaimpg  accountservice   1/1       someprefix/accountservice
</code></pre></div></div>

<p>Call it using curl:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; curl $ManagerIP:6767/accounts/10000
{"id":"10000","name":"Person_0","servedBy":"10.255.0.5"}
</code></pre></div></div>

<p>Lovely. We see that the response now contains the IP address of the container that served our request. Let’s scale the service up a bit:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; docker service scale accountservice=3
accountservice scaled to 3
</code></pre></div></div>

<p>Wait a few seconds and run:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; docker service ls
ID            NAME             REPLICAS  IMAGE
yim6dgzaimpg  accountservice   3/3       someprefix/accountservice
</code></pre></div></div>

<p>Now it says replicas 3/3. Let’s curl a few times and see if get different IP addresses as <em>servedBy</em>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl $ManagerIP:6767/accounts/10000
{"id":"10000","name":"Person_0","servedBy":"10.0.0.22"}

curl $ManagerIP:6767/accounts/10000
{"id":"10000","name":"Person_0","servedBy":"10.255.0.5"}

curl $ManagerIP:6767/accounts/10000
{"id":"10000","name":"Person_0","servedBy":"10.0.0.18"}

curl $ManagerIP:6767/accounts/10000
{"id":"10000","name":"Person_0","servedBy":"10.0.0.22"}
</code></pre></div></div>

<p>We see how our four calls were round-robined over the three instances before 10.0.0.22 got to handle another request. This kind of load-balancing provided by the container orchestrator using the Docker Swarm “service” abstraction is very attractive as it removes the complexity of client-side based load-balancing such as Netflix Ribbon and also shows that we can load-balance without having to rely on a service discovery mechanism to provide us with a list of possible IP-addresses we could call. Also - from Docker 1.13 Docker Swarm won’t route any traffic to nodes not reporting themselves as “healthy” if you have implemented the Healthcheck. This is very important when having to scale up and down a lot, especially if your services are complex and may take more than the few hundreds of milliseconds to start our “accountservice” currently needs.</p>

<h1 id="footprint-and-performance-when-scaling">Footprint and performance when scaling</h1>
<p>It may be interesting to see if and how scaling our accountservice from one to four instances affects latencies and CPU/memory usage. Could there be a substantial overhead when the Swarm mode load-balancer round-robins our requests?</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; docker service scale accountservice=4
</code></pre></div></div>

<p>Give it a few seconds to start things up.</p>

<h2 id="cpu-and-memory-usage-during-load-test">CPU and memory usage during load test</h2>
<p>Running the Gatling test with 1K req/s:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CONTAINER                                    CPU %               MEM USAGE / LIMIT       
accountservice.3.y8j1imkor57nficq6a2xf5gkc   12.69%              9.336 MiB / 1.955 GiB 
accountservice.2.3p8adb2i87918ax3age8ah1qp   11.18%              9.414 MiB / 1.955 GiB 
accountservice.4.gzglenb06bmb0wew9hdme4z7t   13.32%              9.488 MiB / 1.955 GiB 
accountservice.1.y3yojmtxcvva3wa1q9nrh9asb   11.17%              31.26 MiB / 1.955 GiB
</code></pre></div></div>

<p>Well well! Our 4 instances are more or less evenly sharing the workload and we also see that the three “new” instances stay below 10 mb of RAM given that they never should need to serve more than 250 req/s each.</p>

<h2 id="performance">Performance</h2>
<p>First - the Gatling excerpt using one (1) instance:
<img src="../../../../../../assets/blogg/goblog/part6-performance-1.png" alt="performance" />
Next - from the run with four (4) instances:
<img src="../../../../../../assets/blogg/goblog/part6-performance-4.png" alt="performance" /></p>

<p>The difference isn’t all that great - and it shouldn’t be - all four service instances are after all running on the <em>same</em> virtualbox-hosted Docker Swarm node on the same underlying hardware (i.e. my laptop). If we would add more virtualized instances to the Swarm that can utilize <em>unused</em> resources from the host OS we’d probably see a much larger decrease in latencies as it would be separate logical CPUs etc. handling the load. Nevertheless - we do see a slight performance increase regarding the mean and 95/99-percentiles. We can safely conclude that the Swarm mode load-balancing has no negative impact on performance in this particular scenario.</p>

<h1 id="bring-out-the-quotes">Bring out the quotes!</h1>
<p>Remember that Java-based <em>quotes-service</em> we deployed back in <a href="../../../03/09/go-blog-series-part5.html">part 5</a>? Let’s scale it up and then call it from the “accountservice” using its service name “quotes-service”. The purpose of adding this call is to showcase how transparent the service discovery and load-balancing becomes when the only thing we need to know about the service we’re calling is its logical <em>service</em> name.</p>

<p>We’ll start by editing <em>/goblog/accountservice/model/account.go</em> so our response will contain a quote:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> type Account struct {
         Id string `json:"id"`
         Name string  `json:"name"`
         ServedBy string `json:"servedBy"`
         Quote Quote `json:"quote"`         // NEW
 }
 
 // NEW struct
 type Quote struct {
         Text string `json:"quote"`
         ServedBy string `json:"ipAddress"`
         Language string `json:"language"`
 }
</code></pre></div></div>

<p>Note that we’re using the <a href="https://github.com/golang/go/wiki/Well-known-struct-tags">json tags</a> to map from the field names that the <em>quotes-service</em> outputs to struct names of our own, <em>quote</em> to <em>text</em>, <em>ipAddress</em> to <em>ServedBy</em> etc.</p>

<p>Continue by editing <em>/goblog/accountservice/service/handler.go</em>. We’ll add a simplistic <em>getQuote</em> function that will perform a HTTP call to <em>http://quotes-service:8080/api/quote</em> whose return value will be used to populate the new <em>Quote</em> struct. We’ll call it from the main <em>GetAccount</em> handler function.</p>

<p>First, we’ll deal with a Connection: Keep-Alive issue that will cause load-balancing problems unless we explicitly configure the Go http client appropriately. In <em>handlers.go</em>, add the following just above the <em>GetAccount</em> function:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var client = &amp;http.Client{}

func init() {
        var transport http.RoundTripper = &amp;http.Transport{
                DisableKeepAlives: true,
        }
        client.Transport = transport
}
</code></pre></div></div>

<p>This init method will make sure any outgoing HTTP request issued by the <em>client</em> instance will have the appropriate headers making the Docker Swarm-based load-balancing work as expected. Next, just below the <em>GetAccount</em> function, add the package-scoped <em>getQuote()</em> function:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>func getQuote() (model.Quote, error) {
        req, _ := http.NewRequest("GET", "http://quotes-service:8080/api/quote?strength=4", nil)
        resp, err := client.Do(req)

        if err == nil &amp;&amp; resp.StatusCode == 200 {
                quote := model.Quote{}
                bytes, _ := ioutil.ReadAll(resp.Body)
                json.Unmarshal(bytes, &amp;quote)
                return quote, nil
        } else {
                return model.Quote{}, fmt.Errorf("Some error")
        }
}
</code></pre></div></div>

<p>Nothing special about it. That “?strength=4” argument is a peculiarity of the quotes-service API that can be used to make it consume more or less CPU. If there are some problem with the request, we return a generic error.</p>

<p>We’ll call the new <em>getQuote</em> func from the <em>GetAccount</em> function, assigning the returned value to the <em>Quote</em> property of the <em>Account</em> instance if there were no error:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Read the account struct BoltDB
account, err := DBClient.QueryAccount(accountId)
account.ServedBy = getIP()

// NEW call the quotes-service
quote, err := getQuote()
if err == nil {
        account.Quote = quote
}
</code></pre></div></div>

<p>(All this error-checking is one of my least favourite things about Go, even though it arguably produces code that is safer and maybe shows the intent of the code more clearly.)</p>

<h1 id="unit-testing-with-outgoing-http-requests">Unit testing with outgoing HTTP requests</h1>
<p>If we would run the unit tests in <em>/accountservice/service/handlers_test.go</em> now, they would fail! The GetAccount function under test will now try to do a HTTP request to fetch a famous quote, but since there’s no quotes-service running on the specified URL (I guess it won’t resolve to anything) the test cannot pass.</p>

<p>We have two strategies to choose from here given the context of unit testing:</p>

<p>1) Extract the <em>getQuote</em> function into an interface and provide one real and one mock implementation, just like we did in <a href="../../../03/03/go-blog-series-part4.html">part 4</a> for the Bolt client.
2) Utilize a HTTP-specific mocking framework that intercepts outgoing requests for us and returns a pre-determined answer. The built-in httptest package can start an embedded HTTP server for us that can be used for unit-testing, but I’d like to use the 3rd party <a href="https://github.com/h2non/gock">gock</a> framework instead that’s more concise and perhaps a bit easier to use.</p>

<p>In <em>/goblog/accountservice/service/handlers_test.go</em>, add an init function above the <em>TestGetAccount(t *testing)</em> function that will make sure our http <a href="https://github.com/callistaenterprise/goblog/blob/P7/accountservice/service/handlers.go#L20"><em>client</em></a> instance is intercepted properly by gock:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>func init() {
        gock.InterceptClient(client)
}
</code></pre></div></div>

<p>The gock DSL provides fine-granular control over expected outgoing HTTP requests and responses. In the example below, we use New(..), Get(..) and MatchParam(..) to tell gock to expect the <em>http://quotes-service:8080/api/quote?strength=4</em> GET request and respond with HTTP 200 and a hard-coded JSON string as body.</p>

<p>At the top of <em>TestGetAccount(t *testing)</em>, add:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>func TestGetAccount(t *testing.T) {
        defer gock.Off()
        gock.New("http://quotes-service:8080").
                Get("/api/quote").
                MatchParam("strength", "4").
                Reply(200).
                BodyString(`{"quote":"May the source be with you. Always.","ipAddress":"10.0.0.5:8080","language":"en"}`)
</code></pre></div></div>

<p><em>defer gock.Off()</em> makes sure our test will turn off HTTP intercepts <em>after</em> the current test finishes since the <em>gock.New(..)</em> will turn http intercept on which could potentially fail subsequent tests.</p>

<p>Let’s assert that the expected quote was returned. In the innermost <em>Convey</em>-block of the <em>TestGetAccount</em> test, add a new assertion:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Convey("Then the response should be a 200", func() {
        So(resp.Code, ShouldEqual, 200)

        account := model.Account{}
        json.Unmarshal(resp.Body.Bytes(), &amp;account)
        So(account.Id, ShouldEqual, "123")
        So(account.Name, ShouldEqual, "Person_123")
        
        // NEW!
        So(account.Quote.Text, ShouldEqual, "May the source be with you. Always.")
})
</code></pre></div></div>

<h3 id="run-the-tests">Run the tests</h3>
<p>Try running all tests from the <em>/goblog/accountservice</em> folder:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; go test ./...
?   	github.com/callistaenterprise/goblog/accountservice	[no test files]
?   	github.com/callistaenterprise/goblog/accountservice/dbclient	[no test files]
?   	github.com/callistaenterprise/goblog/accountservice/model	[no test files]
ok  	github.com/callistaenterprise/goblog/accountservice/service	0.011s
</code></pre></div></div>

<h3 id="deploy-and-run-this-on-the-swarm">Deploy and run this on the Swarm</h3>
<p>Rebuild/redeploy using <em>./copyall.sh</em> and then try calling the <em>accountservice</em> using curl:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; curl $ManagerIP:6767/accounts/10000
  {"id":"10000","name":"Person_0","servedBy":"10.255.0.8","quote":
      {"quote":"You, too, Brutus?","ipAddress":"461caa3cef02/10.0.0.5:8080","language":"en"}
  }
</code></pre></div></div>

<p>Scale the quotes-service to two instances:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; docker service scale quotes-service=2
</code></pre></div></div>

<p>Give it some time, it may take 15-30 seconds as the Spring Boot-based quotes-service is not as fast as our Go counterparts to start. Then call it again a few times using curl, the result should be something like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{"id":"10000","name":"Person_0","servedBy":"10.255.0.15","quote":{"quote":"To be or not to be","ipAddress":"768e4b0794f6/10.0.0.8:8080","language":"en"}}
{"id":"10000","name":"Person_0","servedBy":"10.255.0.16","quote":{"quote":"Bring out the gimp.","ipAddress":"461caa3cef02/10.0.0.5:8080","language":"en"}}
{"id":"10000","name":"Person_0","servedBy":"10.0.0.9","quote":{"quote":"You, too, Brutus?","ipAddress":"768e4b0794f6/10.0.0.8:8080","language":"en"}}
</code></pre></div></div>

<p>We see that our own <em>servedBy</em> is nicely cycling through the available <em>accountservice</em> instances. We also see that the <em>ipAddress</em> field of the <em>quote</em> object has two different IPs. If we hadn’t disabled the keep-alive behaviour, we’d probably be seeing that the same instance of <em>accountservice</em> keeps serving quotes from the same <em>quotes-service</em> instance.</p>

<h1 id="summary">Summary</h1>
<p>In this part we touched upon the concepts of Service Discovery and Load-balancing in the microservice context and implemented calling of another service using only its logical service name.</p>

<p>In <a href="../../../05/15/go-blog-series-part8.html">part 8</a>, we’ll move on to one of the most important aspects of running microservices at scale - centralized configuration.</p>









            <div class="ce-blog-thanks">
              Tack för att du läser Callistas blogg. <br>
              Hjälp oss att nå ut med information genom att dela nyheter och artiklar i ditt nätverk.<br>
            </div>

            


    
<div class="ce-blog-share">
    <a class="ce-blog-share-element social-twitter" href="https://twitter.com/intent/tweet?text=Go Blog Series Part7&url=https://callistaenterprise.se/blogg/teknik/2017/04/24/go-blog-series-part7/&via=callistaent&hashtags=" target="_blank" title="Share this post on Twitter"></a>
    <a class="ce-blog-share-element social-facebook" href="https://www.facebook.com/sharer/sharer.php?t=Go Blog Series Part7&u=https://callistaenterprise.se/blogg/teknik/2017/04/24/go-blog-series-part7/" target="_blank" title="Share this post on Facebook"></a>
    <a class="ce-blog-share-element social-linkedin" href="http://www.linkedin.com/shareArticle?mini=true&title=Go Blog Series Part7&url=https://callistaenterprise.se/blogg/teknik/2017/04/24/go-blog-series-part7/&source=http%3a%2f%2fzhangwenli.com" target="_blank" title="Share this post on LinkedIn"></a>
</div>
    
        </article>
    </div>
</section>


<section class="ce-section">
    <heading>
        <h1>Kommentarer</h1>
    </heading>
    <div class="ce-content">
        <div id="disqus_thread"></div>
        <script src="../../../../../../js/disqus.js"></script>
    </div>
</section>


    </div>

    <footer class="ce-footer">
    <div class="ce-wrapper">
        <nav class="ce-menu-footer-primary">
            <h2><a href="../../../../../../index.html">Callista</a></h2>
            <ul>
                
                <li><a href="../../../../../../om/index.html">Om oss</a></li>
                
                <li><a href="../../../../../../erbjudanden.html">Erbjudanden</a></li>
                
                <li><a href="../../../../../../event.html">Event</a></li>
                
                <li><a href="../../../../../../blogg.html">Blogg</a></li>
                
                <li><a href="../../../../../../om/jobb/index.html">Jobba hos oss</a></li>
                

                
                    
                        <li><a href="../../../../../../english/index.html">English</a></li>
                    
                
            </ul>
        </nav>
        <div class="ce-addresses">
            <div class="ce-address">
                <h2>Stockholm</h2>
                <address>
                    Drottninggatan 55<br>
                    111 21 Stockholm<br>
                    Tel:
                    <a href="tel:+468212142">
                        +46 8 21 21 42
                    </a>
                </address>
            </div>
            <div class="ce-address">
                <h2>Göteborg</h2>
                <address>
                    Fabriksgatan 13<br>
                    412 50 Göteborg<br>
                    Tel:
                    <a href="tel:+4631201918">
                        +46 31 20 19 18
                    </a>
                </address>
            </div>
        </div>
        <div class="ce-social">
            <h2>Följ oss</h2>
            <ul>
                <li>
                    <a href="http://twitter.com/callistaent">
                        <img alt="Twitters logotype" src="../../../../../../images/icons/twitter.png">
                        <span>@callistaent</span>
                    </a>
                </li>
            </ul>
        </div>
        <small>&copy; 2021 Callista Enterprise AB</small>
        <hr style="margin: 1em 0 2em 0">
        <div class="ce-footer-images">
          <div class="ce-footer-image">
            <a class="ce-raddabarnen" href="http://www.raddabarnen.se/foretag">
                <img alt="Callista är Rädda Barnens företagsvän 2021" src="../../../../../../images/logotype/rb_vanforetag_2021_large-sv.png">
            </a>
          </div>
        </div>
        <br>
        <div class="ce-footer-images">
          <div class="ce-footer-image">
            <a class="ce-di-gasell" href="http://www.di.se/gasell/">
              <img alt="Callista utsett till DI Gasell tre år i rad" src="../../../../../../images/logotype/di_gasell_180x194.png">
            </a>
          </div>
        </div>
    </div>
</footer>


    <script src="../../../../../../js/lib/jquery.min.js"></script>
    <script src="../../../../../../js/lib/owl.carousel.min.js"></script>
    <script src="../../../../../../js/lib/prismjs.min.js"></script>
    <script src="../../../../../../js/app.js"></script>
    <script src="../../../../../../js/analytics.js"></script>
    <script src="../../../../../../js/jobinterestsubmit.js"></script>

</body>
</html>
